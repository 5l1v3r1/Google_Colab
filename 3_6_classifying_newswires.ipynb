{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.6-classifying-newswires.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/alzayats/Google_Colab/blob/master/3_6_classifying_newswires.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "IjFPj6J5EVrI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8c0c4c06-8779-4601-94e1-11b4728bb0cd"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.6'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "VFG6jPbMEVsZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classifying newswires: a multi-class classification example\n",
        "\n",
        "This notebook contains the code samples found in Chapter 3, Section 5 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
        "\n",
        "----\n",
        "\n",
        "In the previous section we saw how to classify vector inputs into two mutually exclusive classes using a densely-connected neural network. \n",
        "But what happens when you have more than two classes? \n",
        "\n",
        "In this section, we will build a network to classify Reuters newswires into 46 different mutually-exclusive topics. Since we have many \n",
        "classes, this problem is an instance of \"multi-class classification\", and since each data point should be classified into only one \n",
        "category, the problem is more specifically an instance of \"single-label, multi-class classification\". If each data point could have \n",
        "belonged to multiple categories (in our case, topics) then we would be facing a \"multi-label, multi-class classification\" problem."
      ]
    },
    {
      "metadata": {
        "id": "HrpMNOifEVsg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The Reuters dataset\n",
        "\n",
        "\n",
        "We will be working with the _Reuters dataset_, a set of short newswires and their topics, published by Reuters in 1986. It's a very simple, \n",
        "widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each \n",
        "topic has at least 10 examples in the training set.\n",
        "\n",
        "Like IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let's take a look right away:"
      ]
    },
    {
      "metadata": {
        "id": "UxO0_djNEVsy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "collapsed": true,
        "outputId": "485de1de-3b74-468c-eda8-7db7b90ea1cb"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import reuters\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MUUGbbg_EVti",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Like with the IMDB dataset, the argument `num_words=10000` restricts the data to the 10,000 most frequently occurring words found in the \n",
        "data.\n",
        "\n",
        "We have 8,982 training examples and 2,246 test examples:"
      ]
    },
    {
      "metadata": {
        "id": "F7QQhEmhEVtx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3e1528d-dc31-42ae-c7ad-50edcbc6d6c1"
      },
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8982"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "j2CANhCMEVuV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50aeb682-3133-4677-947f-f8fd8f1bac30"
      },
      "cell_type": "code",
      "source": [
        "len(test_data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2246"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "xMgHS0vFEVv4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As with the IMDB reviews, each example is a list of integers (word indices):"
      ]
    },
    {
      "metadata": {
        "id": "HoXwMZxvEVwB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "a802d7b8-2efa-4d11-b560-34e9ca924aff"
      },
      "cell_type": "code",
      "source": [
        "train_data[10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 245,\n",
              " 273,\n",
              " 207,\n",
              " 156,\n",
              " 53,\n",
              " 74,\n",
              " 160,\n",
              " 26,\n",
              " 14,\n",
              " 46,\n",
              " 296,\n",
              " 26,\n",
              " 39,\n",
              " 74,\n",
              " 2979,\n",
              " 3554,\n",
              " 14,\n",
              " 46,\n",
              " 4689,\n",
              " 4329,\n",
              " 86,\n",
              " 61,\n",
              " 3499,\n",
              " 4795,\n",
              " 14,\n",
              " 61,\n",
              " 451,\n",
              " 4329,\n",
              " 17,\n",
              " 12]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "LlqaijTaEVwv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here's how you can decode it back to words, in case you are curious:"
      ]
    },
    {
      "metadata": {
        "id": "g_HKUhQ9EVwz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "collapsed": true,
        "outputId": "17a0efb9-2714-4db2-b7ff-cd94da6b7276"
      },
      "cell_type": "code",
      "source": [
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "# Note that our indices were offset by 3\n",
        "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
        "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json\n",
            "557056/550378 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_ObWWYnAEVxl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8252bfae-4ac9-4fdb-f051-204c16bb72c1"
      },
      "cell_type": "code",
      "source": [
        "decoded_newswire"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "vXui891lEVyT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The label associated with an example is an integer between 0 and 45: a topic index."
      ]
    },
    {
      "metadata": {
        "id": "VFIOF2vSEVyf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f57063f-ad47-4ce5-bf55-bcbc71ead5ca"
      },
      "cell_type": "code",
      "source": [
        "train_labels[10]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "Yu7S5d1_EVy9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the data\n",
        "\n",
        "We can vectorize the data with the exact same code as in our previous example:"
      ]
    },
    {
      "metadata": {
        "id": "TyE7mmSlEVzJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "collapsed": true,
        "outputId": "752884ec-6e7c-4565-dd27-eb3e057426c9"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)\n",
        "\n",
        "x_train,x_train.shape,x_train.ndim"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0., 1., 1., ..., 0., 0., 0.],\n",
              "        [0., 1., 1., ..., 0., 0., 0.],\n",
              "        [0., 1., 1., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 1., 1., ..., 0., 0., 0.],\n",
              "        [0., 1., 1., ..., 0., 0., 0.],\n",
              "        [0., 1., 1., ..., 0., 0., 0.]]), (8982, 10000), 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "f5yuqXaJEV0w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "collapsed": true,
        "outputId": "bba39e56-c58e-4393-f43f-3f260463209d"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "\n",
        "# # Our vectorized training data\n",
        "# x_train = vectorize_sequences(train_data)\n",
        "# # Our vectorized test data\n",
        "# x_test = vectorize_sequences(test_data)\n",
        "# x_train[0],x_train.shape,x_train.ndim\n",
        "data=[1,2,4,9,0]\n",
        "x= vectorize_sequences(data)\n",
        "x,x.shape,x.ndim"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), (5, 10), 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "L5smTtglEV1z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "To vectorize the labels, there are two possibilities: we could just cast the label list as an integer tensor, or we could use a \"one-hot\" \n",
        "encoding. One-hot encoding is a widely used format for categorical data, also called \"categorical encoding\". \n",
        "For a more detailed explanation of one-hot encoding, you can refer to Chapter 6, Section 1. \n",
        "In our case, one-hot encoding of our labels consists in embedding each label as an all-zero vector with a 1 in the place of the label index, e.g.:"
      ]
    },
    {
      "metadata": {
        "id": "jwhiyyePEV16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "collapsed": true,
        "outputId": "aaf0e205-fd80-4ae9-bd13-5ab6abc3b28f"
      },
      "cell_type": "code",
      "source": [
        "def to_one_hot(labels, dimension=46):\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "# Our vectorized training labels\n",
        "one_hot_train_labels = to_one_hot(train_labels)\n",
        "# Our vectorized test labels\n",
        "one_hot_test_labels = to_one_hot(test_labels)\n",
        "one_hot_train_labels,one_hot_train_labels.shape,one_hot_train_labels.ndim"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]), (8982, 46), 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "Dxvh0NxGEV2X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that there is a built-in way to do this in Keras, which you have already seen in action in our MNIST example:"
      ]
    },
    {
      "metadata": {
        "id": "KVk7v5MtEV2a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xg_C2kDAEV3D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building our network\n",
        "\n",
        "\n",
        "This topic classification problem looks very similar to our previous movie review classification problem: in both cases, we are trying to \n",
        "classify short snippets of text. There is however a new constraint here: the number of output classes has gone from 2 to 46, i.e. the \n",
        "dimensionality of the output space is much larger. \n",
        "\n",
        "In a stack of `Dense` layers like what we were using, each layer can only access information present in the output of the previous layer. \n",
        "If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each \n",
        "layer can potentially become an \"information bottleneck\". In our previous example, we were using 16-dimensional intermediate layers, but a \n",
        "16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, \n",
        "permanently dropping relevant information.\n",
        "\n",
        "For this reason we will use larger layers. Let's go with 64 units:"
      ]
    },
    {
      "metadata": {
        "id": "lUuPUvsGEV3I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5liom-efEV3c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "There are two other things you should note about this architecture:\n",
        "\n",
        "* We are ending the network with a `Dense` layer of size 46. This means that for each input sample, our network will output a \n",
        "46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n",
        "* The last layer uses a `softmax` activation. You have already seen this pattern in the MNIST example. It means that the network will \n",
        "output a _probability distribution_ over the 46 different output classes, i.e. for every input sample, the network will produce a \n",
        "46-dimensional output vector where `output[i]` is the probability that the sample belongs to class `i`. The 46 scores will sum to 1.\n",
        "\n",
        "The best loss function to use in this case is `categorical_crossentropy`. It measures the distance between two probability distributions: \n",
        "in our case, between the probability distribution output by our network, and the true distribution of the labels. By minimizing the \n",
        "distance between these two distributions, we train our network to output something as close as possible to the true labels."
      ]
    },
    {
      "metadata": {
        "id": "syFOsv2GEV3k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NMY2ameEEV35",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Validating our approach\n",
        "\n",
        "Let's set apart 1,000 samples in our training data to use as a validation set:"
      ]
    },
    {
      "metadata": {
        "id": "ooPf6YeOEV3_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "\n",
        "y_val = one_hot_train_labels[:1000]\n",
        "partial_y_train = one_hot_train_labels[1000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KdSCAy8jEV4U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's train our network for 20 epochs:"
      ]
    },
    {
      "metadata": {
        "id": "1WACW1nQEV4Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "1787ed69-93ca-4875-85cb-bf7973b61eae"
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "7982/7982 [==============================] - 1s 148us/step - loss: 2.5351 - acc: 0.4974 - val_loss: 1.7268 - val_acc: 0.6110\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 1s 123us/step - loss: 1.4517 - acc: 0.6872 - val_loss: 1.3523 - val_acc: 0.7070\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 1s 152us/step - loss: 1.1007 - acc: 0.7631 - val_loss: 1.1734 - val_acc: 0.7430\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 1s 149us/step - loss: 0.8735 - acc: 0.8151 - val_loss: 1.0847 - val_acc: 0.7590\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 1s 148us/step - loss: 0.7063 - acc: 0.8472 - val_loss: 0.9861 - val_acc: 0.7820\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 1s 147us/step - loss: 0.5691 - acc: 0.8790 - val_loss: 0.9405 - val_acc: 0.8040\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 1s 147us/step - loss: 0.4602 - acc: 0.9047 - val_loss: 0.9082 - val_acc: 0.8030\n",
            "Epoch 8/20\n",
            "7982/7982 [==============================] - 1s 148us/step - loss: 0.3710 - acc: 0.9232 - val_loss: 0.9364 - val_acc: 0.7910\n",
            "Epoch 9/20\n",
            "6144/7982 [======================>.......] - ETA: 0s - loss: 0.3060 - acc: 0.9312"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "7982/7982 [==============================] - 1s 151us/step - loss: 0.3046 - acc: 0.9310 - val_loss: 0.8902 - val_acc: 0.8070\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 1s 148us/step - loss: 0.2555 - acc: 0.9411 - val_loss: 0.9051 - val_acc: 0.8110\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 1s 150us/step - loss: 0.2196 - acc: 0.9474 - val_loss: 0.9172 - val_acc: 0.8120\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 1s 151us/step - loss: 0.1887 - acc: 0.9506 - val_loss: 0.9075 - val_acc: 0.8130\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 1s 150us/step - loss: 0.1710 - acc: 0.9525 - val_loss: 0.9329 - val_acc: 0.8090\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 1s 150us/step - loss: 0.1547 - acc: 0.9554 - val_loss: 0.9651 - val_acc: 0.8050\n",
            "Epoch 15/20\n",
            "7982/7982 [==============================] - 1s 149us/step - loss: 0.1397 - acc: 0.9559 - val_loss: 0.9705 - val_acc: 0.8140\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 1s 151us/step - loss: 0.1322 - acc: 0.9557 - val_loss: 1.0261 - val_acc: 0.8000\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 1s 150us/step - loss: 0.1226 - acc: 0.9573 - val_loss: 1.0206 - val_acc: 0.7960\n",
            "Epoch 18/20\n",
            "1536/7982 [====>.........................] - ETA: 0s - loss: 0.1218 - acc: 0.9583"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "7982/7982 [==============================] - 1s 150us/step - loss: 0.1205 - acc: 0.9578 - val_loss: 1.0435 - val_acc: 0.8050\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 1s 148us/step - loss: 0.1148 - acc: 0.9593 - val_loss: 1.0987 - val_acc: 0.7970\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 1s 150us/step - loss: 0.1122 - acc: 0.9598 - val_loss: 1.0735 - val_acc: 0.8000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IE6wqFCVEV4x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's display its loss and accuracy curves:"
      ]
    },
    {
      "metadata": {
        "id": "zr63EK-hEV44",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "7608eeb4-97b5-4250-8c7e-1a29d2861a2a"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss) + 1)\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdb80e391d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vpHOx6FyEV5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "752e8397-30a7-4f9a-ea2b-08556151a763"
      },
      "cell_type": "code",
      "source": [
        "plt.clf()   # clear figure\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFnCAYAAACPasF4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fdb810b24e0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "nTeK5vcoEV5p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It seems that the network starts overfitting after 8 epochs. Let's train a new network from scratch for 8 epochs, then let's evaluate it on \n",
        "the test set:"
      ]
    },
    {
      "metadata": {
        "id": "-NBwTu_aEV50",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "91acc08a-ee4d-46a6-bbdc-62c9c93cb71c"
      },
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=8,\n",
        "          batch_size=512,\n",
        "          validation_data=(x_val, y_val))\n",
        "results = model.evaluate(x_test, one_hot_test_labels)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/8\n",
            "7982/7982 [==============================] - 1s 173us/step - loss: 2.5377 - acc: 0.5233 - val_loss: 1.6804 - val_acc: 0.6540\n",
            "Epoch 2/8\n",
            "7982/7982 [==============================] - 1s 149us/step - loss: 1.3774 - acc: 0.7110 - val_loss: 1.2804 - val_acc: 0.7200\n",
            "Epoch 3/8\n",
            "7982/7982 [==============================] - 1s 148us/step - loss: 1.0200 - acc: 0.7769 - val_loss: 1.1314 - val_acc: 0.7500\n",
            "Epoch 4/8\n",
            "7982/7982 [==============================] - 1s 148us/step - loss: 0.8022 - acc: 0.8246 - val_loss: 1.0561 - val_acc: 0.7520\n",
            "Epoch 5/8\n",
            "7982/7982 [==============================] - 1s 149us/step - loss: 0.6416 - acc: 0.8626 - val_loss: 0.9765 - val_acc: 0.7950\n",
            "Epoch 6/8\n",
            "7982/7982 [==============================] - 1s 148us/step - loss: 0.5134 - acc: 0.8933 - val_loss: 0.9120 - val_acc: 0.8120\n",
            "Epoch 7/8\n",
            "7982/7982 [==============================] - 1s 149us/step - loss: 0.4138 - acc: 0.9154 - val_loss: 0.8950 - val_acc: 0.8240\n",
            "Epoch 8/8\n",
            "7982/7982 [==============================] - 1s 147us/step - loss: 0.3374 - acc: 0.9278 - val_loss: 0.8760 - val_acc: 0.8260\n",
            "2246/2246 [==============================] - 0s 108us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2ZTRk9UhEV6L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27f1b289-b520-47db-fef8-dbf9f18a005f"
      },
      "cell_type": "code",
      "source": [
        "results"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9862981821316645, 0.784060552092609]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "vLBDqTKiEV6l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Our approach reaches an accuracy of ~78%. With a balanced binary classification problem, the accuracy reached by a purely random classifier \n",
        "would be 50%, but in our case it is closer to 19%, so our results seem pretty good, at least when compared to a random baseline:"
      ]
    },
    {
      "metadata": {
        "id": "rL4siZbQEV6x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99d4730f-9e9c-4017-80fe-9771a707c959"
      },
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "test_labels_copy = copy.copy(test_labels)\n",
        "np.random.shuffle(test_labels_copy)\n",
        "float(np.sum(np.array(test_labels) == np.array(test_labels_copy))) / len(test_labels)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19679430097951914"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "8NhcWd4rEV7c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generating predictions on new data\n",
        "\n",
        "We can verify that the `predict` method of our model instance returns a probability distribution over all 46 topics. Let's generate topic \n",
        "predictions for all of the test data:"
      ]
    },
    {
      "metadata": {
        "id": "_-Oewit6EV7z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O3a-KFoAEV83",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Each entry in `predictions` is a vector of length 46:"
      ]
    },
    {
      "metadata": {
        "id": "IWDe-7gYEV87",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "85cf50d8-54c0-4dfb-abb8-34f503110bb6"
      },
      "cell_type": "code",
      "source": [
        "predictions[0].shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(46,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "7_kOl_mcEV9g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The coefficients in this vector sum to 1:"
      ]
    },
    {
      "metadata": {
        "id": "DFIqNRZvEV9j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdcce78e-99fc-45db-c491-1795f79e710c"
      },
      "cell_type": "code",
      "source": [
        "np.sum(predictions[0])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "wm1cKJ9REV98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The largest entry is the predicted class, i.e. the class with the highest probability:"
      ]
    },
    {
      "metadata": {
        "id": "euVbBGvNEV-K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27b4e292-0355-43fa-c38f-b1dbc34e5707"
      },
      "cell_type": "code",
      "source": [
        "np.argmax(predictions[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "hgDM76T-EV-o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A different way to handle the labels and the loss\n",
        "\n",
        "We mentioned earlier that another way to encode the labels would be to cast them as an integer tensor, like such:"
      ]
    },
    {
      "metadata": {
        "id": "JwDSpE-7EV-r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "collapsed": true,
        "outputId": "91d1b24a-a94f-4d1a-9d13-a11c4809ed55"
      },
      "cell_type": "code",
      "source": [
        "y_train = np.array(train_labels)\n",
        "y_test = np.array(test_labels)\n",
        "y_train,y_train.shape,y_train.ndim"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 3,  4,  3, ..., 25,  3, 25]), (8982,), 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "rf5WcVyGEV_Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "The only thing it would change is the choice of the loss function. Our previous loss, `categorical_crossentropy`, expects the labels to \n",
        "follow a categorical encoding. With integer labels, we should use `sparse_categorical_crossentropy`:"
      ]
    },
    {
      "metadata": {
        "id": "ko7KMZIlEV_V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CsEv2y5NEV_s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This new loss function is still mathematically the same as `categorical_crossentropy`; it just has a different interface."
      ]
    },
    {
      "metadata": {
        "id": "8eL2Kcx-EV_0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## On the importance of having sufficiently large intermediate layers\n",
        "\n",
        "\n",
        "We mentioned earlier that since our final outputs were 46-dimensional, we should avoid intermediate layers with much less than 46 hidden \n",
        "units. Now let's try to see what happens when we introduce an information bottleneck by having intermediate layers significantly less than \n",
        "46-dimensional, e.g. 4-dimensional."
      ]
    },
    {
      "metadata": {
        "id": "m2qd9nkUEV_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "outputId": "4f271e59-26c7-4300-887f-7e7bf418347f"
      },
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(4, activation='relu'))\n",
        "model.add(layers.Dense(46, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=20,\n",
        "          batch_size=128,\n",
        "          validation_data=(x_val, y_val))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7982 samples, validate on 1000 samples\n",
            "Epoch 1/20\n",
            "7982/7982 [==============================] - 1s 183us/step - loss: 2.7061 - acc: 0.4402 - val_loss: 2.0142 - val_acc: 0.5930\n",
            "Epoch 2/20\n",
            "7982/7982 [==============================] - 2s 190us/step - loss: 1.7335 - acc: 0.6185 - val_loss: 1.6567 - val_acc: 0.6110\n",
            "Epoch 3/20\n",
            "7982/7982 [==============================] - 2s 202us/step - loss: 1.4672 - acc: 0.6358 - val_loss: 1.5692 - val_acc: 0.6090\n",
            "Epoch 4/20\n",
            "7982/7982 [==============================] - 2s 195us/step - loss: 1.3265 - acc: 0.6427 - val_loss: 1.4830 - val_acc: 0.6160\n",
            "Epoch 5/20\n",
            "7982/7982 [==============================] - 2s 201us/step - loss: 1.2162 - acc: 0.6496 - val_loss: 1.4554 - val_acc: 0.6220\n",
            "Epoch 6/20\n",
            "7982/7982 [==============================] - 2s 206us/step - loss: 1.1256 - acc: 0.6755 - val_loss: 1.4326 - val_acc: 0.6340\n",
            "Epoch 7/20\n",
            "7982/7982 [==============================] - 2s 198us/step - loss: 1.0509 - acc: 0.6937 - val_loss: 1.4515 - val_acc: 0.6380\n",
            "Epoch 8/20\n",
            " 128/7982 [..............................] - ETA: 1s - loss: 1.0886 - acc: 0.6719"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "7982/7982 [==============================] - 2s 199us/step - loss: 0.9910 - acc: 0.7036 - val_loss: 1.4421 - val_acc: 0.6510\n",
            "Epoch 9/20\n",
            "7982/7982 [==============================] - 2s 204us/step - loss: 0.9402 - acc: 0.7179 - val_loss: 1.4839 - val_acc: 0.6500\n",
            "Epoch 10/20\n",
            "7982/7982 [==============================] - 2s 208us/step - loss: 0.8973 - acc: 0.7428 - val_loss: 1.5093 - val_acc: 0.6530\n",
            "Epoch 11/20\n",
            "7982/7982 [==============================] - 2s 201us/step - loss: 0.8584 - acc: 0.7563 - val_loss: 1.5112 - val_acc: 0.6650\n",
            "Epoch 12/20\n",
            "7982/7982 [==============================] - 2s 202us/step - loss: 0.8239 - acc: 0.7618 - val_loss: 1.5252 - val_acc: 0.6650\n",
            "Epoch 13/20\n",
            "7982/7982 [==============================] - 2s 200us/step - loss: 0.7945 - acc: 0.7707 - val_loss: 1.5487 - val_acc: 0.6670\n",
            "Epoch 14/20\n",
            "7982/7982 [==============================] - 2s 203us/step - loss: 0.7680 - acc: 0.7775 - val_loss: 1.5972 - val_acc: 0.6670\n",
            "Epoch 15/20\n",
            "2304/7982 [=======>......................] - ETA: 1s - loss: 0.7446 - acc: 0.7713"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "7982/7982 [==============================] - 2s 204us/step - loss: 0.7436 - acc: 0.7863 - val_loss: 1.6376 - val_acc: 0.6720\n",
            "Epoch 16/20\n",
            "7982/7982 [==============================] - 2s 201us/step - loss: 0.7194 - acc: 0.7955 - val_loss: 1.7237 - val_acc: 0.6660\n",
            "Epoch 17/20\n",
            "7982/7982 [==============================] - 2s 203us/step - loss: 0.7010 - acc: 0.8029 - val_loss: 1.7239 - val_acc: 0.6730\n",
            "Epoch 18/20\n",
            "7982/7982 [==============================] - 2s 209us/step - loss: 0.6810 - acc: 0.8078 - val_loss: 1.7835 - val_acc: 0.6720\n",
            "Epoch 19/20\n",
            "7982/7982 [==============================] - 2s 211us/step - loss: 0.6647 - acc: 0.8108 - val_loss: 1.7769 - val_acc: 0.6810\n",
            "Epoch 20/20\n",
            "7982/7982 [==============================] - 2s 206us/step - loss: 0.6479 - acc: 0.8165 - val_loss: 1.8367 - val_acc: 0.6760\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdb80e439b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "qM3WI9P_EWAg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "Our network now seems to peak at ~71% test accuracy, a 8% absolute drop. This drop is mostly due to the fact that we are now trying to \n",
        "compress a lot of information (enough information to recover the separation hyperplanes of 46 classes) into an intermediate space that is \n",
        "too low-dimensional. The network is able to cram _most_ of the necessary information into these 8-dimensional representations, but not all \n",
        "of it."
      ]
    },
    {
      "metadata": {
        "id": "ATpb_WHfEWAn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Further experiments\n",
        "\n",
        "* Try using larger or smaller layers: 32 units, 128 units...\n",
        "* We were using two hidden layers. Now try to use a single hidden layer, or three hidden layers."
      ]
    },
    {
      "metadata": {
        "id": "tYR-aCvMEWAq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrapping up\n",
        "\n",
        "\n",
        "Here's what you should take away from this example:\n",
        "\n",
        "* If you are trying to classify data points between N classes, your network should end with a `Dense` layer of size N.\n",
        "* In a single-label, multi-class classification problem, your network should end with a `softmax` activation, so that it will output a \n",
        "probability distribution over the N output classes.\n",
        "* _Categorical crossentropy_ is almost always the loss function you should use for such problems. It minimizes the distance between the \n",
        "probability distributions output by the network, and the true distribution of the targets.\n",
        "* There are two ways to handle labels in multi-class classification:\n",
        "    ** Encoding the labels via \"categorical encoding\" (also known as \"one-hot encoding\") and using `categorical_crossentropy` as your loss \n",
        "function.\n",
        "    ** Encoding the labels as integers and using the `sparse_categorical_crossentropy` loss function.\n",
        "* If you need to classify data into a large number of categories, then you should avoid creating information bottlenecks in your network by having \n",
        "intermediate layers that are too small."
      ]
    }
  ]
}